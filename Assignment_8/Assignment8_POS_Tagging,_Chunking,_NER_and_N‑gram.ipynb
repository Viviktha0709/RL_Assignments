{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a98ff488",
      "metadata": {
        "id": "a98ff488"
      },
      "source": [
        "# POS Tagging, Chunking, NER and N‑gram Language Model in Python\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **POS Tagging** (Part-of-Speech tagging)\n",
        "2. **Syntactic Chunking** (e.g., noun phrase chunks)\n",
        "3. **Named Entity Recognition (NER)**\n",
        "4. **N‑gram Language Model** (unigram, bigram, trigram)\n",
        "\n",
        "We will use the **NLTK** library for basic NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DE0pfai5AqB",
        "outputId": "c4066ef4-fbbc-4c28-8d31-5b1e5fbd21df"
      },
      "id": "-DE0pfai5AqB",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fd3ee31f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd3ee31f",
        "outputId": "e1c7fab2-4ed6-43e9-dc14-aad70f85ec5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download required NLTK resources (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import RegexpParser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc676bb6",
      "metadata": {
        "id": "cc676bb6"
      },
      "source": [
        "## 1. POS Tagging\n",
        "\n",
        "In this section we:\n",
        "1. Take an input sentence.\n",
        "2. Tokenize it into words.\n",
        "3. Apply NLTK's `pos_tag` to get part‑of‑speech tags."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# NEW tagger model\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# These two are for NER\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ9Qyy_a5Hhi",
        "outputId": "d92b21b7-23e3-4502-d578-ed61ef891a59"
      },
      "id": "fQ9Qyy_a5Hhi",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ea656a35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea656a35",
        "outputId": "d81a2228-1ed5-4a77-e202-98a4c8881c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['John', 'bought', 'a', 'new', 'laptop', 'from', 'the', 'market', 'yesterday', '.']\n",
            "\n",
            "POS Tags (word, tag):\n",
            "[('John', 'NNP'), ('bought', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('laptop', 'NN'), ('from', 'IN'), ('the', 'DT'), ('market', 'NN'), ('yesterday', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Example sentence for POS tagging\n",
        "sentence = \"John bought a new laptop from the market yesterday.\"\n",
        "\n",
        "# Step 1: Tokenize\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Step 2: POS Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"\\nPOS Tags (word, tag):\")\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f12c309",
      "metadata": {
        "id": "0f12c309"
      },
      "source": [
        "## 2. Chunking (Syntactic Parsing)\n",
        "\n",
        "Chunking groups words into **phrases** like noun phrases (NP).\n",
        "We define a simple chunk grammar and apply it using NLTK's `RegexpParser`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7693becb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7693becb",
        "outputId": "d1b988eb-93e8-4c84-d8f9-995cb6f08c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP John/NNP)\n",
            "  bought/VBD\n",
            "  (NP a/DT new/JJ laptop/NN)\n",
            "  from/IN\n",
            "  (NP the/DT market/NN yesterday/NN)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "# Define a simple chunk grammar for Noun Phrases (NP)\n",
        "# DT = determiner, JJ = adjective, NN/NNS/NNP/NNPS = noun\n",
        "chunk_grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN.*>+}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# Use the POS‑tagged sentence from above\n",
        "chunk_tree = chunk_parser.parse(pos_tags)\n",
        "print(chunk_tree)\n",
        "\n",
        "# If you run this in a local Jupyter environment with a GUI, you can visualize the tree as:\n",
        "# chunk_tree.draw()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "865aead8",
      "metadata": {
        "id": "865aead8"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "\n",
        "We use NLTK's built‑in `ne_chunk` which performs NER on POS‑tagged tokens.\n",
        "It can recognize entities like **PERSON**, **ORGANIZATION**, **GPE** (Geo‑Political Entity), etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Core tokenizers & tagger\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# NER models (both are needed in new NLTK)\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "# Word list (used by NER)\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smIoHtKw5R1K",
        "outputId": "f656e475-b515-48c1-962a-d6ee8861ca53"
      },
      "id": "smIoHtKw5R1K",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "318cdef3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318cdef3",
        "outputId": "d3669a96-5ae3-4b62-8804-8e6e3f56f682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Barack', 'Obama', 'was', 'the', '44th', 'President', 'of', 'the', 'United', 'States', 'of', 'America', '.']\n",
            "\n",
            "POS Tags:\n",
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('44th', 'JJ'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('of', 'IN'), ('America', 'NNP'), ('.', '.')]\n",
            "\n",
            "Named Entity Tree:\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  of/IN\n",
            "  (GPE America/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "# Example sentence for NER\n",
        "sentence_ner = \"Barack Obama was the 44th President of the United States of America.\"\n",
        "tokens_ner = word_tokenize(sentence_ner)\n",
        "pos_tags_ner = pos_tag(tokens_ner)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens_ner)\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_tags_ner)\n",
        "\n",
        "# Perform NER\n",
        "ner_tree = ne_chunk(pos_tags_ner)\n",
        "print(\"\\nNamed Entity Tree:\")\n",
        "print(ner_tree)\n",
        "\n",
        "# You can also visualize with ner_tree.draw() in a local environment\n",
        "# ner_tree.draw()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9250e8",
      "metadata": {
        "id": "2d9250e8"
      },
      "source": [
        "## 4. N‑gram Language Model\n",
        "\n",
        "An **N‑gram language model** estimates the probability of a word given the previous *(N‑1)* words.\n",
        "\n",
        "We will:\n",
        "1. Create a small corpus of sentences.\n",
        "2. Tokenize and add start `<s>` and end `</s>` markers.\n",
        "3. Build **unigram**, **bigram** and **trigram** counts.\n",
        "4. Convert counts to probabilities.\n",
        "5. Generate text from the bigram/trigram model.\n",
        "\n",
        "This is a simple **Maximum Likelihood Estimation (MLE)** model without smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "830f5b31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "830f5b31",
        "outputId": "13618f90-e856-4285-8012-ae7877c26658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'john', 'likes', 'to', 'watch', 'movies', '</s>']\n",
            "['<s>', 'mary', 'likes', 'movies', 'too', '</s>']\n",
            "['<s>', 'john', 'also', 'likes', 'to', 'watch', 'football', '</s>']\n",
            "['<s>', 'mary', 'enjoys', 'football', 'and', 'movies', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Small training corpus (you can replace these with any sentences)\n",
        "corpus_sentences = [\n",
        "    \"John likes to watch movies\",\n",
        "    \"Mary likes movies too\",\n",
        "    \"John also likes to watch football\",\n",
        "    \"Mary enjoys football and movies\"\n",
        "]\n",
        "\n",
        "def prepare_corpus(sentences):\n",
        "    \"\"\"Tokenize each sentence and add <s> and </s> markers.\"\"\"\n",
        "    tokenized_sentences = []\n",
        "    for sent in sentences:\n",
        "        tokens = word_tokenize(sent.lower())\n",
        "        # Add start and end tokens\n",
        "        tokenized_sentences.append([\"<s>\"] + tokens + [\"</s>\"])\n",
        "    return tokenized_sentences\n",
        "\n",
        "tokenized_corpus = prepare_corpus(corpus_sentences)\n",
        "for ts in tokenized_corpus:\n",
        "    print(ts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b351338b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b351338b",
        "outputId": "1b247e01-f2d2-4114-8f72-5a53038fc1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram probabilities for context ('likes',):\n",
            "{'to': 0.6666666666666666, 'movies': 0.3333333333333333}\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n=2):\n",
        "    \"\"\"Build n‑gram counts: context (tuple) -> Counter of next words.\"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    for sent in tokenized_sentences:\n",
        "        if len(sent) < n:\n",
        "            continue\n",
        "        for i in range(len(sent) - n + 1):\n",
        "            ngram = sent[i:i + n]\n",
        "            context = tuple(ngram[:-1])  # previous n-1 words\n",
        "            next_word = ngram[-1]\n",
        "            counts[context][next_word] += 1\n",
        "    return counts\n",
        "\n",
        "def counts_to_probabilities(counts):\n",
        "    \"\"\"Convert counts to probability distributions.\"\"\"\n",
        "    probs = {}\n",
        "    for context, counter in counts.items():\n",
        "        total = float(sum(counter.values()))\n",
        "        probs[context] = {w: c / total for w, c in counter.items()}\n",
        "    return probs\n",
        "\n",
        "# Build bigram (n=2) and trigram (n=3) models\n",
        "bigram_counts = build_ngram_counts(tokenized_corpus, n=2)\n",
        "bigram_probs = counts_to_probabilities(bigram_counts)\n",
        "\n",
        "trigram_counts = build_ngram_counts(tokenized_corpus, n=3)\n",
        "trigram_probs = counts_to_probabilities(trigram_counts)\n",
        "\n",
        "print(\"Bigram probabilities for context ('likes',):\")\n",
        "print(bigram_probs.get((\"likes\",), {}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "653beb59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653beb59",
        "outputId": "46f67f5e-442e-4b26-d0c7-ba0bfc7e3f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence with bigram model:\n",
            "john also likes to watch movies\n",
            "\n",
            "Generated sentence with trigram model:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_from_ngram(probs, n=2, max_length=15, start_token='<s>'):\n",
        "    \"\"\"Generate a sentence from an n‑gram model.\n",
        "    probs: context -> {next_word: prob}\n",
        "    n: order of the model\n",
        "    \"\"\"\n",
        "    if n == 1:\n",
        "        # Unigram model: ignore context\n",
        "        context = ()\n",
        "    else:\n",
        "        # Start with <s> repeated n-1 times\n",
        "        context = tuple([start_token] * (n - 1))\n",
        "\n",
        "    result = []\n",
        "    for _ in range(max_length):\n",
        "        context_probs = probs.get(context, None)\n",
        "        if not context_probs:\n",
        "            break\n",
        "        # Sample next word according to probability distribution\n",
        "        words = list(context_probs.keys())\n",
        "        p = list(context_probs.values())\n",
        "        next_word = random.choices(words, weights=p, k=1)[0]\n",
        "\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "        result.append(next_word)\n",
        "\n",
        "        if n > 1:\n",
        "            # Update context (slide window by 1)\n",
        "            context = tuple(list(context)[1:] + [next_word])\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Example: generate text using bigram model\n",
        "print(\"Generated sentence with bigram model:\")\n",
        "print(generate_from_ngram(bigram_probs, n=2))\n",
        "\n",
        "# Example: generate text using trigram model\n",
        "print(\"\\nGenerated sentence with trigram model:\")\n",
        "print(generate_from_ngram(trigram_probs, n=3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}